# ChatAgent Async Streaming Example

This example demonstrates the **TRUE asynchronous streaming** capabilities of `ChatAgent` in LangGraphGo, using the LLM's native streaming API to receive responses in real-time as they're being generated.

## Features

- **Real LLM Streaming**: `AsyncChat` uses `llms.WithStreamingFunc` for genuine streaming from the model
- **Low Latency**: First tokens appear immediately, not after complete generation
- **Token-Level Chunks**: Receives chunks as the LLM generates them (typically words or sub-words)
- **Context Support**: Full support for context cancellation and timeouts
- **Non-Blocking**: Returns immediately with a channel for receiving results
- **Easy Integration**: Simple channel-based API that's idiomatic in Go

## Why Async Streaming?

Traditional chat interfaces wait for the complete response before displaying anything to the user. Async streaming provides several benefits:

1. **Better UX**: Users see responses appearing in real-time
2. **Perceived Performance**: Feels faster even if actual processing time is the same
3. **Natural Interaction**: Mimics human conversation where responses appear gradually
4. **Early Feedback**: Users can start reading before the full response is complete
5. **Interruptible**: Can cancel long responses that aren't relevant

## API Overview

### AsyncChat - Character Streaming

```go
respChan, err := agent.AsyncChat(ctx, "Hello!")
if err != nil {
    log.Fatal(err)
}

for char := range respChan {
    fmt.Print(char)  // Print each character as it arrives
}
```

### AsyncChatWithChunks - Word Streaming

```go
respChan, err := agent.AsyncChatWithChunks(ctx, "Explain AI")
if err != nil {
    log.Fatal(err)
}

for word := range respChan {
    fmt.Print(word)  // Print each word as it arrives
}
```

## How It Works

1. **Call Method**: Call `AsyncChat` with your message
2. **Get Channel**: Receive a read-only channel of strings
3. **Start Reading**: Immediately start reading from the channel
4. **Receive Chunks**: Tokens/chunks arrive as the LLM generates them in real-time
5. **Channel Closes**: Channel closes automatically when response is complete

Behind the scenes (`AsyncChat`):
- A goroutine is spawned to handle the streaming
- The LLM's `GenerateContent` is called with `llms.WithStreamingFunc`
- As the LLM generates each token/chunk, it's immediately sent to the callback
- The callback forwards chunks to the channel in real-time
- The channel is closed when the LLM finishes generation
- The complete response is saved to conversation history

**This is TRUE streaming** - chunks arrive as they're generated by the LLM, not after buffering the complete response!

## Running the Example

```bash
cd examples/chat_agent_async
go run main.go
```

## Example Output

```
=== ChatAgent AsyncChat Demo ===

--- Demo 1: Character-by-Character Streaming ---
User: Hello!
Agent: H e l l o !   I ' m   a n   A I   a s s i s t a n t . . .

--- Demo 2: Word-by-Word Streaming ---
User: Can you explain async chat?
Agent: Of course! I can explain async chat...

--- Demo 3: Collecting Full Response ---
User: What's the benefit of streaming?
Agent: The main benefit is improved perceived performance...
[Received 45 chunks, total length: 234 characters]
```

## Advanced Usage

### Context Cancellation

```go
// Create a context with timeout
ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
defer cancel()

respChan, err := agent.AsyncChat(ctx, message)
for chunk := range respChan {
    // Stream will stop when context is cancelled
    fmt.Print(chunk)
}
```

### Collecting Full Response

```go
var fullResponse string
for chunk := range respChan {
    fullResponse += chunk
}
// fullResponse now contains the complete text
```

### Adding Delays for Effect

```go
for chunk := range respChan {
    fmt.Print(chunk)
    time.Sleep(50 * time.Millisecond)  // Simulate typing effect
}
```

## Comparison with Regular Chat

| Feature | Regular Chat | AsyncChat | AsyncChatWithChunks |
|---------|--------------|-----------|---------------------|
| Returns | Complete response | Channel | Channel |
| Streaming | No | Character-level | Word-level |
| Blocking | Yes | No | No |
| Context Support | Yes | Yes | Yes |
| Use Case | Simple requests | Typing effect | Readable streaming |

## Integration with Real LLMs

When using with real LLMs that support streaming (like OpenAI):

```go
import "github.com/tmc/langchaingo/llms/openai"

model, _ := openai.New(openai.WithModel("gpt-4"))
agent, _ := prebuilt.NewChatAgent(model, nil)

// Async streaming works with any LLM
respChan, _ := agent.AsyncChatWithChunks(ctx, "Explain quantum computing")
for word := range respChan {
    fmt.Print(word)
}
```

## Notes

- **Goroutines**: Each async call spawns one goroutine that cleans up automatically
- **Memory**: Buffered channel (100 capacity) prevents blocking on slow consumers
- **Errors**: Errors during processing cause the channel to close early
- **Thread Safety**: Safe to call from multiple goroutines
- **Conversation History**: History is maintained normally, just like regular `Chat`

## Best Practices

1. **Always drain the channel**: Read until it closes to prevent goroutine leaks
2. **Use context timeouts**: Prevent infinite waits on slow responses
3. **Choose appropriate method**:
   - `AsyncChat` for character-by-character typing effects
   - `AsyncChatWithChunks` for more natural word-by-word streaming
4. **Handle context cancellation**: Check for closed channels when using cancellable contexts

## See Also

- [Basic ChatAgent Example](../chat_agent/) - Simple multi-turn conversation
- [Dynamic Tools Example](../chat_agent_dynamic_tools/) - Runtime tool management
- [ChatAgent Documentation](../../prebuilt/CHAT_AGENT.md) - Complete API reference
