package prebuilt

import (
	"context"
	"fmt"
	"strings"

	"github.com/smallnest/langgraphgo/graph"
	"github.com/smallnest/langgraphgo/log"
	"github.com/tmc/langchaingo/llms"
)

// CreateReflectionAgentTyped creates a new Reflection Agent with full type safety.
// The agent iteratively improves its responses through self-reflection.
//
// The Reflection pattern involves:
// 1. Generate: Create an initial response
// 2. Reflect: Critique the response and suggest improvements
// 3. Revise: Generate an improved version based on reflection
// 4. Repeat until satisfactory or max iterations reached
func CreateReflectionAgentTyped(config ReflectionAgentConfig) (*graph.StateRunnable[ReflectionAgentState], error) {
	if config.Model == nil {
		return nil, fmt.Errorf("model is required")
	}

	if config.MaxIterations == 0 {
		config.MaxIterations = 3 // Default to 3 iterations
	}

	// Use same model for reflection if not specified
	reflectionModel := config.ReflectionModel
	if reflectionModel == nil {
		reflectionModel = config.Model
	}

	// Default system messages
	if config.SystemMessage == "" {
		config.SystemMessage = "You are a helpful assistant. Generate a high-quality response to the user's request."
	}

	if config.ReflectionPrompt == "" {
		config.ReflectionPrompt = buildDefaultReflectionPromptTyped()
	}

	// Create the workflow with generic state type
	workflow := graph.NewStateGraph[ReflectionAgentState]()

	// Define state schema for merging
	schema := graph.NewStructSchema(
		ReflectionAgentState{},
		func(current, new ReflectionAgentState) (ReflectionAgentState, error) {
			// Append new messages to current messages
			current.Messages = append(current.Messages, new.Messages...)
			// Overwrite other fields
			current.Iteration = new.Iteration
			current.Reflection = new.Reflection
			current.Draft = new.Draft
			return current, nil
		},
	)
	workflow.SetSchema(schema)

	// Add generation node
	workflow.AddNode("generate", "Generate or revise response based on reflection", func(ctx context.Context, state ReflectionAgentState) (ReflectionAgentState, error) {
		return generateNodeTyped(ctx, state, config.Model, config.SystemMessage, config.Verbose)
	})

	// Add reflection node
	workflow.AddNode("reflect", "Reflect on the generated response and suggest improvements", func(ctx context.Context, state ReflectionAgentState) (ReflectionAgentState, error) {
		return reflectNodeTyped(ctx, state, reflectionModel, config.ReflectionPrompt, config.Verbose)
	})

	// Set entry point
	workflow.SetEntryPoint("generate")

	// Add conditional edge from generate
	workflow.AddConditionalEdge("generate", func(ctx context.Context, state ReflectionAgentState) string {
		return shouldContinueAfterGenerateTyped(state, config.MaxIterations, config.Verbose)
	})

	// Add conditional edge from reflect
	workflow.AddConditionalEdge("reflect", func(ctx context.Context, state ReflectionAgentState) string {
		return shouldContinueAfterReflectTyped(state, config.Verbose)
	})

	return workflow.Compile()
}

// generateNodeTyped generates or revises a response (typed version)
func generateNodeTyped(ctx context.Context, state ReflectionAgentState, model llms.Model, systemMessage string, verbose bool) (ReflectionAgentState, error) {
	// Get messages
	if len(state.Messages) == 0 {
		return state, fmt.Errorf("no messages in state")
	}

	// Build prompt based on iteration
	var promptMessages []llms.MessageContent

	if state.Iteration == 0 {
		// First generation
		if verbose {
			log.Info("generating initial response...")
		}

		promptMessages = []llms.MessageContent{
			{
				Role:  llms.ChatMessageTypeSystem,
				Parts: []llms.ContentPart{llms.TextPart(systemMessage)},
			},
		}
		promptMessages = append(promptMessages, state.Messages...)
	} else {
		// Revision based on reflection
		if state.Reflection == "" {
			return state, fmt.Errorf("no reflection found for revision")
		}

		if verbose {
			log.Info("revising response (iteration %d)...", state.Iteration)
		}

		// Construct revision prompt
		revisionPrompt := fmt.Sprintf(`You are revising your previous response based on reflection.

Original request:
%s

Previous draft:
%s

Reflection and suggestions for improvement:
%s

Generate an improved response that addresses the issues raised in the reflection.`,
			getOriginalRequestTyped(state.Messages), state.Draft, state.Reflection)

		promptMessages = []llms.MessageContent{
			{
				Role:  llms.ChatMessageTypeSystem,
				Parts: []llms.ContentPart{llms.TextPart(systemMessage)},
			},
			{
				Role:  llms.ChatMessageTypeHuman,
				Parts: []llms.ContentPart{llms.TextPart(revisionPrompt)},
			},
		}
	}

	// Generate response
	resp, err := model.GenerateContent(ctx, promptMessages)
	if err != nil {
		return state, fmt.Errorf("failed to generate response: %w", err)
	}

	draft := resp.Choices[0].Content

	if verbose {
		log.Info("draft generated (%d chars)", len(draft))
	}

	// Create AI message
	aiMsg := llms.MessageContent{
		Role:  llms.ChatMessageTypeAI,
		Parts: []llms.ContentPart{llms.TextPart(draft)},
	}

	return ReflectionAgentState{
		Messages:   []llms.MessageContent{aiMsg},
		Draft:      draft,
		Iteration:  state.Iteration + 1,
		Reflection: state.Reflection,
	}, nil
}

// reflectNodeTyped reflects on the generated response (typed version)
func reflectNodeTyped(ctx context.Context, state ReflectionAgentState, model llms.Model, reflectionPrompt string, verbose bool) (ReflectionAgentState, error) {
	if state.Draft == "" {
		return state, fmt.Errorf("no draft found to reflect on")
	}

	if len(state.Messages) == 0 {
		return state, fmt.Errorf("no messages in state")
	}

	originalRequest := getOriginalRequestTyped(state.Messages)

	if verbose {
		log.Info("reflecting on the response...")
	}

	// Build reflection prompt
	reflectionMessages := []llms.MessageContent{
		{
			Role:  llms.ChatMessageTypeSystem,
			Parts: []llms.ContentPart{llms.TextPart(reflectionPrompt)},
		},
		{
			Role: llms.ChatMessageTypeHuman,
			Parts: []llms.ContentPart{
				llms.TextPart(fmt.Sprintf(`Original request:
%s

Generated response:
%s

Provide a critical reflection on this response.`, originalRequest, state.Draft)),
			},
		},
	}

	// Generate reflection
	resp, err := model.GenerateContent(ctx, reflectionMessages)
	if err != nil {
		return state, fmt.Errorf("failed to generate reflection: %w", err)
	}

	reflection := resp.Choices[0].Content

	if verbose {
		log.Info("reflection:\n%s\n", reflection)
	}

	return ReflectionAgentState{
		Messages:    state.Messages,
		Iteration:   state.Iteration,
		Draft:       state.Draft,
		Reflection:  reflection,
	}, nil
}

// shouldContinueAfterGenerateTyped decides whether to reflect or end (typed version)
func shouldContinueAfterGenerateTyped(state ReflectionAgentState, maxIterations int, verbose bool) string {
	// On first iteration, always reflect
	if state.Iteration == 1 {
		return "reflect"
	}

	// If we've reached max iterations, stop
	if state.Iteration >= maxIterations {
		if verbose {
			log.Info("max iterations reached, finalizing response")
		}
		return graph.END
	}

	// Check if previous reflection was satisfactory
	isSatisfactory := isResponseSatisfactoryTyped(state.Reflection)
	if isSatisfactory {
		if verbose {
			log.Info("response is satisfactory, finalizing")
		}
		return graph.END
	}

	// Continue reflecting
	return "reflect"
}

// shouldContinueAfterReflectTyped decides whether to revise or accept (typed version)
func shouldContinueAfterReflectTyped(state ReflectionAgentState, verbose bool) string {
	isSatisfactory := isResponseSatisfactoryTyped(state.Reflection)

	if isSatisfactory {
		if verbose {
			log.Info("reflection indicates response is satisfactory")
		}
		return graph.END
	}

	// Revise based on reflection
	return "generate"
}

// isResponseSatisfactory analyzes the reflection to determine if response is good
func isResponseSatisfactoryTyped(reflection string) bool {
	reflectionLower := strings.ToLower(reflection)

	// Keywords indicating satisfaction
	satisfactoryKeywords := []string{
		"excellent",
		"satisfactory",
		"no major issues",
		"no significant issues",
		"well done",
		"comprehensive",
		"thorough",
		"accurate",
		"no improvements needed",
		"meets all requirements",
	}

	// Keywords indicating issues
	issueKeywords := []string{
		"missing",
		"incomplete",
		"unclear",
		"should include",
		"could be improved",
		"lacks",
		"needs to",
		"issue",
		"problem",
		"incorrect",
		"inaccurate",
	}

	satisfactoryCount := 0
	issueCount := 0

	// Check satisfactory keywords first (including longer phrases)
	for _, keyword := range satisfactoryKeywords {
		if strings.Contains(reflectionLower, keyword) {
			satisfactoryCount++
		}
	}

	// Check issue keywords, but exclude if they're part of a satisfactory phrase
	// For example, "issue" in "no major issues" shouldn't count as negative
	for _, keyword := range issueKeywords {
		if strings.Contains(reflectionLower, keyword) {
			// Check if this keyword is part of a satisfactory phrase
			isPartOfSatisfactory := false
			for _, satKeyword := range satisfactoryKeywords {
				if strings.Contains(satKeyword, keyword) && strings.Contains(reflectionLower, satKeyword) {
					isPartOfSatisfactory = true
					break
				}
			}
			if !isPartOfSatisfactory {
				issueCount++
			}
		}
	}

	// If we found satisfactory indicators and no issues, it's good
	if satisfactoryCount > 0 && issueCount == 0 {
		return true
	}

	// If we found more issues than satisfactory indicators, needs improvement
	if issueCount > satisfactoryCount {
		return false
	}

	// Default: continue improving
	return false
}

// getOriginalRequest extracts the original user request from messages
func getOriginalRequestTyped(messages []llms.MessageContent) string {
	for _, msg := range messages {
		if msg.Role == llms.ChatMessageTypeHuman {
			for _, part := range msg.Parts {
				if textPart, ok := part.(llms.TextContent); ok {
					return textPart.Text
				}
			}
		}
	}
	return ""
}

// buildDefaultReflectionPrompt creates the default reflection prompt
func buildDefaultReflectionPromptTyped() string {
	return `You are a critical reviewer providing constructive feedback on AI-generated responses.

Your task is to evaluate the response and provide:
1. Strengths: What the response does well
2. Weaknesses: What could be improved
3. Specific suggestions: Concrete ways to enhance the response

Evaluation criteria:
- Accuracy: Is the information correct and factual?
- Completeness: Does it fully address the request?
- Clarity: Is it well-organized and easy to understand?
- Relevance: Does it stay focused on the topic?
- Quality: Is the writing clear and professional?

Format your reflection as:
**Strengths:**
[List strengths]

**Weaknesses:**
[List weaknesses or write "No major issues"]

**Suggestions for improvement:**
[Specific actionable suggestions or write "No improvements needed"]

Be honest but constructive. If the response is excellent, say so clearly.`
}
